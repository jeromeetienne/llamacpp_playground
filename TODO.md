- [ ] support systemPrompt in _direct and in _langchain
  - **BUG** showhow it doesnt seems to react well
  - is that a bug, or me being crazy or a bug in the model ?
- [ ] currenlty the context is hardcoded in the framework
  - fix that
- [ ] put dataset_evaluation in its own git, 
  - how to import the llamacpp_playground ? can i make a seperate git for that ?
  - it is required for source and to download models
  - git submodule ? no crazy... well documented ?
    - not crazy because it is done only once on a not that important directory
    - doc here https://gist.github.com/gitaarik/8735255
- [ ] perform one prompt optimisation for a given model
- [ ] perform one pick the best model for a prompt

## Done
- [x] download zephyr, llama-13b
  - DONE codellama-7b, llama-7b, codellama-7b, codellama-13b
- [x] in available_models, sort by model size
- [x] DONE update all the jsdoc, in prediction and hptuning
- [x] add npm scripts
- [x] add documentation on the steps
- [x] support predictionName in .hptuning.json5
- [x] rename prompt into userPrompt
